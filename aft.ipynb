{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, x):\n",
    "        proj = self.c_attn(x)\n",
    "        query, key, value = proj.chunk(3, -1)\n",
    "        _, seq_len, _ = query.shape\n",
    "        selected_mask = self.causal_mask[:seq_len, :seq_len]\n",
    "        selected_bias = self.pos_bias[:seq_len, :seq_len]\n",
    "        selected_bias = selected_bias.unsqueeze(0)\n",
    "        masked_bias = selected_bias.masked_fill(~selected_mask, float('-inf'))\n",
    "\n",
    "        # normalize k and bias to prevent numerical instability when taking exp\n",
    "        maxk = key.max(dim=-1, keepdim=True)[0]\n",
    "        key = key - maxk\n",
    "        maxpb = masked_bias.max(dim=-1, keepdim=True)[0]\n",
    "        masked_bias = masked_bias - maxpb\n",
    "\n",
    "        key = torch.exp(key)\n",
    "        expbias = torch.exp(masked_bias)\n",
    "        num = torch.einsum('bij, bjd -> bid', expbias, key * value)\n",
    "        denom = torch.einsum('bij, bjd -> bid', expbias, key)\n",
    "\n",
    "        y = torch.sigmoid(query) * (num / denom)\n",
    "        return self.c_proj(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch, seq_len, dim = 2, 3, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "q,k,v = torch.rand(batch, seq_len, dim), torch.rand(batch, seq_len, dim), torch.rand(batch, seq_len, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.rand(seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.exp(k)\n",
    "mask = mask.unsqueeze(0)\n",
    "mask = torch.exp(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "kv = k * v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = torch.einsum('bij, bjd -> bid', mask, kv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "imperative_num = torch.zeros(batch, seq_len, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in range(batch):\n",
    "    for i in range(seq_len):\n",
    "        for d in range(dim):\n",
    "            total = 0\n",
    "            for j in range(seq_len):\n",
    "                total += mask[0, i, j] * kv[b, j, d]\n",
    "            imperative_num[b, i, d] = total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#check all close\n",
    "print(torch.allclose(num, imperative_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.1534, 1.6675, 2.5779],\n",
       "         [2.2789, 2.5981, 1.2872],\n",
       "         [1.1746, 1.9078, 1.0497]]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.6336, 1.1607, 0.8870, 2.2571],\n",
       "         [0.0812, 0.0491, 0.0094, 0.7999],\n",
       "         [0.7740, 0.4729, 1.8156, 0.7394]],\n",
       "\n",
       "        [[0.8664, 0.2987, 1.2297, 0.8572],\n",
       "         [0.7729, 1.5015, 2.2767, 0.9715],\n",
       "         [0.0795, 0.9834, 1.1632, 0.3573]]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.8617,  2.6400,  5.7191,  5.8435],\n",
       "         [ 2.6513,  3.3816,  4.3828,  8.1739],\n",
       "         [ 1.7117,  1.9536,  2.9657,  4.9536]],\n",
       "\n",
       "        [[ 2.4932,  5.3832,  8.2133,  3.5297],\n",
       "         [ 4.0849,  5.8474, 10.2146,  4.9374],\n",
       "         [ 2.5757,  4.2476,  7.0089,  3.2353]]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally Figured it Out!\n",
    "\n",
    "So for that fancy-lookig einsum, what is really doing is surprisingly simple. Take for instance the following `kv`:\n",
    "\n",
    "        [[0.6336, 1.1607, 0.8870, 2.2571],\n",
    "         [0.0812, 0.0491, 0.0094, 0.7999],\n",
    "         [0.7740, 0.4729, 1.8156, 0.7394]],\n",
    "\n",
    "        [[0.8664, 0.2987, 1.2297, 0.8572],\n",
    "         [0.7729, 1.5015, 2.2767, 0.9715],\n",
    "         [0.0795, 0.9834, 1.1632, 0.3573]]\n",
    "\n",
    "which was a element-wise multiplication of the values in the `k` and `v` matrices.\n",
    "\n",
    "Now suppose we have the following mask:\n",
    "\n",
    "        [[[1.1534, 1.6675, 2.5779],\n",
    "         [2.2789, 2.5981, 1.2872],\n",
    "         [1.1746, 1.9078, 1.0497]]]\n",
    "\n",
    "Recall that in a transformer architecture, we take into account the effects of previous context by taking a weighted sum of the value vectors of the respective tokens. \n",
    "\n",
    "In this case, the first token of the first batch is represented using\n",
    "\n",
    "        [0.6336, 1.1607, 0.8870, 2.2571]\n",
    "embedding vector\n",
    "\n",
    "In the resulting vector after \"attention\" has been applied, the first element is a weighted sum of the 3 vectors:\n",
    "\n",
    "$w_1 (0.6336) + w_2 (0.0812) + w_3 (0.7740) $\n",
    "\n",
    "where 0.0812 comes from\n",
    "\n",
    "[0.0812, 0.0491, 0.0094, 0.7999]\n",
    "and 0.7740 comes from\n",
    "\n",
    "[0.0795, 0.9834, 1.1632, 0.3573]\n",
    "\n",
    "This is what the einsum does.\n",
    "\n",
    "The weights for the first vector will always be from the mask vector\n",
    "\n",
    "[1.1534, 1.6675, 2.5779]\n",
    "so $w_1 = 1.1534$, $w_2 = 1.6675$, and $w_3 = 2.5779$\n",
    "\n",
    "So for example, the second element in the resulting vector for the first token will be:\n",
    "\n",
    "$.1534*1.1607 + 1.6675*0.0491 + 2.5779*0.4729 = 2.6397$\n",
    "\n",
    "For reference, the computed vector is:\n",
    "\n",
    "[ 2.8617,  2.6400,  5.7191,  5.8435]\n",
    "\n",
    "Notice that (for the print out at least), the second vector was rounded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dndai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
